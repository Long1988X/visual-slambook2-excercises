[toc]

# 第6讲 非线性优化

## 1. 目标

1. 理解最小二乘法的含义和处理方式。
2. 理解高斯牛顿法(Gauss-Newton's method)、列文伯格-马夸尔特(Levenburg-Marquadt's method)等下降测略。
3. 学习Ceres和g2o库的基本使用方法。

## 2. 知识点

1. 如何在有噪声的数据中进行准确的状态估计；
2. 最优化背景知识；
3. 批量状态估计：
   1. 贝叶斯法则：最大后验估计和最大似然估计；
   2. 最大似然 ->  最小二乘；
4. 非线性最小二乘：
   1. **迭代的方法**：对于不容易直接求解的最小二乘问题；
   2. 一阶梯度法(最速下降法)：Jacobian Matrix；
   3. 二阶梯度法(牛顿法)：Hessian Matrix；
   4. 高斯牛顿法：**回避H矩阵的求解，用J的表达式近似了H**；
   5. 列文伯格-马夸尔特法：置信区间；
5. 与线性规划不同，非线性需要针对具体问题具体分析；

## 3. 实践

1. 安装Ceres和g2o库；==注意版本，有问题降版本有可能解决。==

2. 学习Tutorials，基本概念、操作、流程；

3. 曲线拟合-手写高斯牛顿法：

   1. ==疑问==：将数据点数量改为1000,但创建x数据时，i / 100, 这个100不动；拟合效果很差！

      > 原因：
      >
      > 目前考虑后面y的值太大了！
      >
      > **后续使用ceres库求解该问题，拟合正常**，说明自己写的曲线拟合方法考虑的不够全面；

4. 曲线拟合-Ceres库：

   > 1. 定义代价函数的计算模型 struct；
   > 2. 构造最小二乘
   >    1.  ceres::Problem 
   >    2. ceres::AutoDiffCostFucntion
   >    3. problem.AddResiduaBlock
   > 3. 配置求解器
   >    1. ceres::Slover::Options
   >    2. ceres::Slover::Summary
   > 4. Ceres求解
   >    1. ceres::Solve(options, &problem, &summary)

5. 曲线拟合-g2o库：

   > 1. 只要一个优化问题能够表达成图，就可以用g2o去求解它；比如：Bundle Adjustment、ICP、数据拟合等;
   > 2. 定义顶点和边的类型；
   > 3. 构建图；
   > 4. 选择优化算法；
   > 5. 调用g2o进行优化，返回结果。

## 4. 课后习题

1. 证明线性方程$Ax=b$当系数矩阵$A$超定时，最小二乘解为$x=(A^TA)^{-1}A^Tb$。

   > ​	[转：最小二乘解](https://blog.csdn.net/kokerf/article/details/72437294)
   >
   > 1. 超定方程：
   >
   >    一般我们会面临形如$A_{m×n}x=b$的方程。我们考虑测量数据和我们需要的解的参数之间的关系，该方程的解可以分为以下几种情况：
   >
   >    > 1. 如果*m*<*n*，未知数大于方程数。那么解不唯一，存在一个解矢量空间。
   >    >
   >    > 2. 如果*m*=*n*，那么只要*A*可逆（非奇异，也就是满秩）就有唯一解，解为$x=A^{-1}b$。
   >    >
   >    > 3. 如果*m*>*n*，方程数大于未知数。方程一般没有解，除非**b**属于*A*的列向量组成的子空间。
   >
   >    我们考虑*m*≥*n*并且*r*(*A*)=*n*的情况。
   >
   >    如果解不存在，我们找一个最接近$A_{m×n}x=b$的解矢量仍然有意义，这个方程成为**超定方程**（方程大于未知数）。
   >
   > 2. 最小二乘解：
   >
   >    我们寻找一个向量**x**使得 ∥*A***x**−**b**∥最小，这里的∥∙∥表示矢量范数。这样的**x**称为该超定方程组的**最小二乘解**。
   >
   > 3. 正规方程：
   >
   >    ![最小二乘](https://img-blog.csdn.net/20170517203432506?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQva29rZXJm/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
   >
   >    几何上我们可以这么理解，要使*A***x**最接近**b**，我们需要使得∥*A***x**−**b**∥最小，也就是让向量*A***x**−**b**垂直*A*的列空间。
   >
   >    也就是说让*A***x**−**b**垂直*A*的每一列，即$a_i^T(Ax-b)=0$，则有 ：
   >
   >    $A^T(Ax-b)=0$ 
   >
   >    把上式括号拆开做整理可以得到:
   >
   >    $A^TAx=A^Tb$
   >
   >    这是一个*n*×*n*的线性方程组，称为**正规方程组**。如果*A*的秩为*n*，那么*A**T**A*的秩也为*n*，上式有解为：
   >
   >    $x=(A^TA)^{-1}A^Tb$

2. 调研最速下降法、牛顿法、高斯牛顿法和列文伯格-马夸尔特方法各有什么优缺点？

   除了我们举的Ceres和g2o库，还有哪些常用的优化库？

   > 1. 最速下降法：
   >
   >    优点：十分简单，直接按照梯度的反方向下降即可；
   >
   >    缺点：过于贪心，容易呈锯齿状下降，从而增加迭代次数；
   >
   > 2. 牛顿法：
   >
   >    优点：相对而言也非常直观，由于引入二阶导数，可以处理一阶导为0的情况；
   >
   >    缺点：二阶导数计算量大；Hessian Matrix
   >
   > 3. 高斯牛顿法：
   >
   >    优点：使用**$J^TJ$**代替Hessian Matrix，避免二阶导数的计算；
   >
   >    缺点：$J^TJ$很容易病态，导致无法得到正确的结果；
   >
   > 4. 列文伯格-马夸尔特法：
   >
   >    优点：引入置信区间和阻尼项，使得$J^TJ$不那么容易病态，并且可以通过调整阻尼完成在梯度法和牛顿法之间切换；
   >
   >    缺点：收敛略慢；

3. 为什么高斯牛顿法的增量方程系数矩阵可能不正定？

   不正定有什么几何含义？

   为什么在这种情况下解就不稳定了？

4. DogLeg是什么？它与高斯牛顿法和列文伯格-马夸尔特方法有何异同？

   > Dogleg属于Trust Region优化方法，即用置信域的方法在最速下降法和高斯牛顿法之间进行切换（将二者的搜索步长及方向转化为向量，两个向量进行叠加得到新的方向和置信域内的步长），相当于是一种加权求解。

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/20181105212051625.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTIzNDg3NzQ=,size_16,color_FFFFFF,t_70)

5. 阅读Ceres的教学材料（http://ceres-solver.org/tutorial.html）以更好的掌握其用法。

   > 1. Curve Fitting
   > 2. BA
   > 3. 例子：无数据的最小二乘 -> 有数据的曲线拟合 -> 有数据(sets of features location and correspondence)的BA，目标：3D point position and camera parameters；

6. 阅读g2o自带的文档，你能看懂它吗？

   如果还不能完全看懂，请在阅读第10讲和第11讲之后再回来看。

7. 请更改曲线拟合实验中的曲线模型，并用Ceres和g2o进行优化实验。例如，可以使用更多的参数和更复杂的模型。